{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cb995f-2830-4184-af1a-4896d93464bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: transformers in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (4.26.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: scikit-learn in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: xxhash in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: packaging in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: aiohttp in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pandas in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers huggingface_hub scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcc52dd-6771-4d40-b137-631765bde194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe0988c-74e5-43ce-a1df-0fdab5bae9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "data_files = {\"train\": data_dir+\"train_sa.csv\", \"dev\": data_dir+\"dev_sa.csv\", \"test\": data_dir+\"test_sa.csv\"}\n",
    "dataset_splits = {}\n",
    "for split, filepath in data_files.items():\n",
    "    df = pd.read_csv(data_dir+filepath)\n",
    "    df = pd.DataFrame(df)\n",
    "    dataset_splits[split] = Dataset.from_pandas(df, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa54eb54-cb75-42a1-ab9c-761462abf12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['review', 'len_review_text', 'review_title', 'review_text', 'overall_rating', 'rating'],\n",
       "     num_rows: 21708\n",
       " }),\n",
       " 'dev': Dataset({\n",
       "     features: ['review', 'len_review_text', 'review_title', 'review_text', 'overall_rating', 'rating'],\n",
       "     num_rows: 9038\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['submission_date', 'reviewer_id', 'product_id', 'product_name', 'site_category_lv1', 'review_title', 'overall_rating', 'recommend_to_a_friend', 'review_text', 'review', 'len_review_text', 'rating'],\n",
       "     num_rows: 22559\n",
       " })}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5112444f-1129-44fe-93f2-8da74eca7b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'N√ÉO RECOMENDO A LOJA √ìTICA SHOP COMO PARCEIRO DA LOJAS AMERICANAS\\nIndependente do \"parceiro\" estou muito decepcionada com a m√°-f√© da lojas americanas.... n√£o ser trata de equ√≠voco, enviar um modelo pr√≥ximo  ao q foi comprado √© equ√≠voco, j√° enviar um modelo q n√£o tem absolutamente nada a ver com o q foi comprado (pedi dourado veio prata, tamanho da caixa, largura pulseira, tudo muito diferente) √© m√° f√©!!!!!!  Ainda mais porque a nota fiscal veio certa, o produto q n√£o.   √â um absurdooooo!!!!!!!!!!!!!!',\n",
       " 'len_review_text': 439,\n",
       " 'review_title': 'N√ÉO RECOMENDO A LOJA √ìTICA SHOP COMO PARCEIRO DA LOJAS AMERICANAS',\n",
       " 'review_text': 'Independente do \"parceiro\" estou muito decepcionada com a m√°-f√© da lojas americanas.... n√£o ser trata de equ√≠voco, enviar um modelo pr√≥ximo  ao q foi comprado √© equ√≠voco, j√° enviar um modelo q n√£o tem absolutamente nada a ver com o q foi comprado (pedi dourado veio prata, tamanho da caixa, largura pulseira, tudo muito diferente) √© m√° f√©!!!!!!  Ainda mais porque a nota fiscal veio certa, o produto q n√£o.   √â um absurdooooo!!!!!!!!!!!!!!',\n",
       " 'overall_rating': 1,\n",
       " 'rating': -1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splits['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34588254-16a0-44f5-9afc-962189dbee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", max_seq_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841810f0-8dd6-43d0-9ab8-6e283df7f3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48914aae135246129328ee1b08ab40f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b04144304f4e758d14cad07161f4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b22d1e0fb3c479fb82c7791c69be650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    sentences_out = tokenizer(examples[\"review\"], truncation=True, max_length=512)\n",
    "    sentences_out['labels'] = []\n",
    "    for rating in examples['rating']:\n",
    "        sentences_out['labels'].append(0 if rating == -1 else 1)\n",
    "    return sentences_out\n",
    " \n",
    "tokenized_train = dataset_splits[\"train\"].shuffle().map(preprocess_function, batched=True)\n",
    "tokenized_dev = dataset_splits[\"dev\"].shuffle().map(preprocess_function, batched=True)\n",
    "tokenized_test = dataset_splits[\"test\"].shuffle().map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add0d251-0609-4b11-9319-662703ff3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ad5701-516e-4171-86c1-fb6363b24957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401e2c99-6f4a-4dcb-8605-8ebc04c5c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "    load_accuracy = load_metric(\"accuracy\")\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "    #load_roc_auc = load_metric(\"roc_auc\")\n",
    "    load_precision = load_metric(\"precision\")\n",
    "    load_recall = load_metric(\"recall\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    print('labels', labels, predictions)\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    #roc_auc = load_roc_auc.compute(prediction_scores=logits, references=labels)[\"roc_auc\"]\n",
    "    precision = load_precision.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = load_recall.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    \n",
    "    return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            #\"roc_auc\": roc_auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(y_true=labels, y_pred=predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d8c867d-2628-43dc-9d44-5867b9eea1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name = \"finetuning-sentiment-model-v0\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repo_name,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = 500,\n",
    "    push_to_hub=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=20,\n",
    "    report_to=[\"wandb\", \"tensorboard\"],\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    #est_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42aa69d2-ace7-4143-8983-1fcaee7a0654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, len_review_text, overall_rating, rating, review_text. If review, review_title, len_review_text, overall_rating, rating, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/eduardo/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 21708\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1356\n",
      "  Number of trainable parameters = 108924674\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meduagarcia\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e/ceia/nlp/projeto_nlp/sentiment_analysis/wandb/run-20230213_153523-y54sc6qn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eduagarcia/huggingface/runs/y54sc6qn' target=\"_blank\">clear-durian-51</a></strong> to <a href='https://wandb.ai/eduagarcia/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eduagarcia/huggingface' target=\"_blank\">https://wandb.ai/eduagarcia/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eduagarcia/huggingface/runs/y54sc6qn' target=\"_blank\">https://wandb.ai/eduagarcia/huggingface/runs/y54sc6qn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1356' max='1356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1356/1356 08:22, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.109493</td>\n",
       "      <td>0.968245</td>\n",
       "      <td>0.977038</td>\n",
       "      <td>0.993977</td>\n",
       "      <td>0.960667</td>\n",
       "      <td>0.973436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.089863</td>\n",
       "      <td>0.973667</td>\n",
       "      <td>0.981186</td>\n",
       "      <td>0.986018</td>\n",
       "      <td>0.976400</td>\n",
       "      <td>0.971794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.094787</td>\n",
       "      <td>0.973445</td>\n",
       "      <td>0.980913</td>\n",
       "      <td>0.991798</td>\n",
       "      <td>0.970264</td>\n",
       "      <td>0.975624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.092501</td>\n",
       "      <td>0.974441</td>\n",
       "      <td>0.981633</td>\n",
       "      <td>0.992284</td>\n",
       "      <td>0.971208</td>\n",
       "      <td>0.976656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.088268</td>\n",
       "      <td>0.974441</td>\n",
       "      <td>0.981686</td>\n",
       "      <td>0.989452</td>\n",
       "      <td>0.974040</td>\n",
       "      <td>0.974716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.094204</td>\n",
       "      <td>0.973445</td>\n",
       "      <td>0.980895</td>\n",
       "      <td>0.992749</td>\n",
       "      <td>0.969320</td>\n",
       "      <td>0.976271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, len_review_text, overall_rating, rating, review_text. If review, review_title, len_review_text, overall_rating, rating, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9038\n",
      "  Batch size = 8\n",
      "/tmp/ipykernel_16878/848210118.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  load_accuracy = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 1 ... 0 0 1] [1 0 1 ... 0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, len_review_text, overall_rating, rating, review_text. If review, review_title, len_review_text, overall_rating, rating, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9038\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 1 ... 0 0 1] [1 0 1 ... 0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to finetuning-sentiment-model-v0/checkpoint-500\n",
      "Configuration saved in finetuning-sentiment-model-v0/checkpoint-500/config.json\n",
      "Model weights saved in finetuning-sentiment-model-v0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in finetuning-sentiment-model-v0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-v0/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, len_review_text, overall_rating, rating, review_text. If review, review_title, len_review_text, overall_rating, rating, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9038\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 1 ... 0 0 1] [1 0 1 ... 0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, len_review_text, overall_rating, rating, review_text. If review, review_title, len_review_text, overall_rating, rating, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9038\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 1 ... 0 0 1] [1 0 1 ... 0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, len_review_text, overall_rating, rating, review_text. If review, review_title, len_review_text, overall_rating, rating, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9038\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 1 ... 0 0 1] [1 0 1 ... 0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to finetuning-sentiment-model-v0/checkpoint-1000\n",
      "Configuration saved in finetuning-sentiment-model-v0/checkpoint-1000/config.json\n",
      "Model weights saved in finetuning-sentiment-model-v0/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in finetuning-sentiment-model-v0/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in finetuning-sentiment-model-v0/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, len_review_text, overall_rating, rating, review_text. If review, review_title, len_review_text, overall_rating, rating, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9038\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 1 ... 0 0 1] [1 0 1 ... 0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1356, training_loss=0.11035913576242846, metrics={'train_runtime': 506.6473, 'train_samples_per_second': 85.693, 'train_steps_per_second': 2.676, 'total_flos': 2515539022533120.0, 'train_loss': 0.11035913576242846, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85cd7ead-b2ea-43d9-b54b-feb31c068f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, len_review_text, overall_rating, rating, review_text. If review, review_title, len_review_text, overall_rating, rating, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9038\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 1 ... 0 0 1] [1 0 1 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09265030920505524,\n",
       " 'eval_accuracy': 0.9734454525337464,\n",
       " 'eval_f1': 0.9809160305343512,\n",
       " 'eval_precision': 0.9916398713826367,\n",
       " 'eval_recall': 0.9704216488357458,\n",
       " 'eval_balanced_accuracy': 0.9755165664014672,\n",
       " 'eval_runtime': 23.543,\n",
       " 'eval_samples_per_second': 383.894,\n",
       " 'eval_steps_per_second': 47.997,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "406c2153-bb11-4f49-a22d-216878c082e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, submission_date, overall_rating, len_review_text, product_name, product_id, rating, reviewer_id, recommend_to_a_friend, site_category_lv1, review_text. If review, review_title, submission_date, overall_rating, len_review_text, product_name, product_id, rating, reviewer_id, recommend_to_a_friend, site_category_lv1, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22559\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 0 ... 1 1 1] [1 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09441617876291275,\n",
       " 'eval_accuracy': 0.973935014849949,\n",
       " 'eval_f1': 0.9812607559436548,\n",
       " 'eval_precision': 0.989650295705837,\n",
       " 'eval_recall': 0.9730122614081659,\n",
       " 'eval_balanced_accuracy': 0.9745571920073337,\n",
       " 'eval_runtime': 57.367,\n",
       " 'eval_samples_per_second': 393.24,\n",
       " 'eval_steps_per_second': 49.157,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93013d83-b510-431d-a5a3-e29debce4b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: review, review_title, submission_date, overall_rating, len_review_text, product_name, product_id, rating, reviewer_id, recommend_to_a_friend, site_category_lv1, review_text. If review, review_title, submission_date, overall_rating, len_review_text, product_name, product_id, rating, reviewer_id, recommend_to_a_friend, site_category_lv1, review_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22559\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1 0 0 ... 1 1 1] [1 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ad211-9483-408b-a19f-69e86d84f588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ada0097-2361-4f3a-bc42-d85baeacf6a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of 'finetuning-sentiment-model-v0/checkpoint-2714/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'finetuning-sentiment-model-v0/checkpoint-2714/' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/transformers/configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:166\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'finetuning-sentiment-model-v0/checkpoint-2714/'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinetuning-sentiment-model-v0/checkpoint-2714/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/transformers/pipelines/__init__.py:675\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 675\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    678\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:852\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    851\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 852\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/transformers/configuration_utils.py:565\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    567\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.10/site-packages/transformers/configuration_utils.py:641\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the configuration of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    646\u001b[0m         )\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load the configuration of 'finetuning-sentiment-model-v0/checkpoint-2714/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'finetuning-sentiment-model-v0/checkpoint-2714/' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('text-classification', model='finetuning-sentiment-model-v0/checkpoint-2714/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43850a-9cfa-49ec-882a-7629bf5c493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe([\"Celular muito bom Super recomendo E sim chegou com 12 dias de anteced√™ncia Muito bom mesmo\", \"Ol√°! N√£o, s√≥ possu√≠mos o modelo com esse tamanho de tela. Permanecemos a disposi√ß√£o!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcacdb6-b083-45af-9ced-59b6d8623451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class CustomBERT(BertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return pooled_output, logits\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finetuning-sentiment-model-v0/checkpoint-1000/\")\n",
    "model = CustomBERT.from_pretrained(\"finetuning-sentiment-model-v0/checkpoint-1000/\")\n",
    "\n",
    "\n",
    "data_dir = '../data/'\n",
    "data_files = [\"train_rec\", \"test\"]\n",
    "for filepath in data_files:\n",
    "    df = pd.read_csv(data_dir+filepath+'.csv')\n",
    "    embeddings = []\n",
    "    scores = []\n",
    "    for text in tqdm(df['review']):\n",
    "        input_text = text\n",
    "        # tokenizer-> token_id\n",
    "        input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=512)\n",
    "        # input_ids: [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "        pooled_output, logits = model(input_ids)\n",
    "\n",
    "        embedding = pooled_output[0].cpu().detach().numpy()\n",
    "        score = torch.nn.functional.softmax(logits[0]).cpu().detach().numpy()[1]\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "        scores.append(score)\n",
    "    df['embedding'] = embeddings\n",
    "    df['scores'] = scores\n",
    "    df.to_parquet(data_dir+filepath+'.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f20812-d03c-4df4-8036-887a93567cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../data/t.parquet')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "957640cc-1cb4-4112-b2a8-b88e40b47906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_ids).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 45,
>>>>>>> 0bd32629847c5354f620d40314beac317dd1ba0b
   "id": "32fa1bdc-7708-4d6c-ac70-35ac5fc2b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(model(input_ids)[1][0]).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146831a5-3cf6-4ce7-a2f4-4247707f158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/test.parquet')\n",
    "df['pred_sentiment'] = df['scores'].round()\n",
    "df['y_sentiment'] = df['rating'].apply(lambda x: 0 if x == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64464a9b-7e3b-4c6e-b29c-6124705e8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['pred_sentiment'] != df['y_sentiment']) & (df['rating'] != 0)][['review_text', 'overall_rating', 'pred_sentiment']].to_csv('analysis.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "34d2ae2012797dca4d79c11a1a7ff6d473285cf12047e136d2cb3fa4cfb42b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
